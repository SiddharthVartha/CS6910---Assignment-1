{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oG4bR3ld4vd"
      },
      "outputs": [],
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rM7z30DGsVH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.datasets import fashion_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "# Load Fashion MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "# Display sample images\n",
        "def display_sample_images():\n",
        "    label = {0: \"T-shirt/top\", 1: \"Trouser\", 2: \"Pullover\", 3: \"Dress\", 4: \"Coat\", 5: \"Sandal\", 6: \"Shirt\", 7: \"Sneaker\", 8: \"Bag\", 9: \"Ankle boot\"}\n",
        "    print_Once = [1] * 10\n",
        "    count = 10\n",
        "    fig, axes = plt.subplots(1, 10, figsize=(20, 4))\n",
        "\n",
        "    for i in range(60000):\n",
        "        if count == 0:\n",
        "            break\n",
        "        if print_Once[y_train[i]]:\n",
        "            print_Once[y_train[i]] -= 1\n",
        "            count -= 1\n",
        "            col = 10 - count\n",
        "            axes[col - 1].imshow(x_train[i], cmap='gray')\n",
        "            axes[col - 1].set_title(\"{}\".format(label[y_train[i]]))\n",
        "            axes[col - 1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    wandb.log({\"Sample Image\":plt})\n",
        "\n",
        "# Initialize weights and biases\n",
        "def initialize_weights(initialization_func, prev_layer_neurons, no_of_hidden_layers, classes,each_layer_neuron):\n",
        "    theta = []\n",
        "    for i in range(2 * (no_of_hidden_layers + 1)):\n",
        "        theta.append([])\n",
        "    for i in range(no_of_hidden_layers):\n",
        "        neurons_in_layer = each_layer_neuron\n",
        "        make_weights(theta, neurons_in_layer, prev_layer_neurons, i, initialization_func)\n",
        "        make_biases(theta, neurons_in_layer, no_of_hidden_layers + 1 + i, initialization_func)\n",
        "        prev_layer_neurons = neurons_in_layer\n",
        "    make_weights(theta, classes, prev_layer_neurons, no_of_hidden_layers, initialization_func)\n",
        "    make_biases(theta, classes, 2 * (no_of_hidden_layers + 1) - 1, initialization_func)\n",
        "    return theta\n",
        "\n",
        "# Make weights\n",
        "def make_weights(theta, curr_layer_neurons, prev_layer_neurons, layer_no, initialization_func):\n",
        "    if initialization_func == \"random\":\n",
        "        theta[layer_no] = np.float128(np.random.randn(curr_layer_neurons, prev_layer_neurons))\n",
        "    elif initialization_func == \"Xavier\":\n",
        "        factor = np.sqrt(6.0 / (curr_layer_neurons + prev_layer_neurons))\n",
        "        theta[layer_no] = np.float128(np.random.uniform(low=-factor, high=factor, size=(curr_layer_neurons, prev_layer_neurons)))\n",
        "\n",
        "# Make biases\n",
        "def make_biases(theta, curr_layer_neurons, layer_no, initialization_func):\n",
        "    if initialization_func == \"random\":\n",
        "        theta[layer_no] = np.float128(np.random.randn(curr_layer_neurons, 1))\n",
        "    elif initialization_func == \"Xavier\":\n",
        "        theta[layer_no] = np.float128(np.zeros((curr_layer_neurons, 1)))\n",
        "\n",
        "# Calculate activation function\n",
        "def calc_activation(a, activation_func):\n",
        "    h = []\n",
        "    a=np.round(a,6)\n",
        "    for i in range(len(a)):\n",
        "        if activation_func == \"sigmoid\":\n",
        "          if(a[i][0]<-30):\n",
        "            h.append(0.0)\n",
        "          else:\n",
        "            h.append(1/(1+np.exp(-a[i][0])))\n",
        "\n",
        "        elif activation_func == \"ReLU\":\n",
        "            h.append(max(0, a[i][0]))\n",
        "\n",
        "        elif activation_func == \"tanh\":\n",
        "           if(a[i][0]<-30):\n",
        "              h.append(-1.0)\n",
        "           else:\n",
        "              h.append(2 * (1 / (1 + np.exp(-2 * a[i][0]))) - 1)\n",
        "        elif activation_func==\"identity\":\n",
        "            h.append(a[i][0])\n",
        "    h = np.array(h)\n",
        "    h_new = h.reshape((len(h), 1))\n",
        "    return h_new\n",
        "\n",
        "# Calculate activation derivative\n",
        "def calc_activation_derivative(a, activation_func):\n",
        "    if activation_func == \"sigmoid\":\n",
        "        a=0.0 if(a<-30) else 1/(1+np.exp(a))\n",
        "        return a * (1 - (a))\n",
        "    elif activation_func == \"ReLU\":\n",
        "        return np.where(a > 0, 1, 0)\n",
        "    elif activation_func == \"tanh\":\n",
        "        return 1-(np.tanh(a)**2)\n",
        "    elif activation_func==\"identity\":\n",
        "        return 1\n",
        "\n",
        "def calc_gdash(ak,activationFunc):\n",
        "  gdsh=[]\n",
        "  for i in ak:\n",
        "      gdsh.append(calc_activation_derivative(i[0],activationFunc))\n",
        "  gdsh=np.array(gdsh)\n",
        "  gdshNew=gdsh.reshape((len(gdsh),1))\n",
        "  return gdshNew\n",
        "\n",
        "def calc_aL(aL,y):\n",
        "  aL[y][0]=-(1-aL[y][0])\n",
        "  return aL\n",
        "\n",
        "# Calculate softmax\n",
        "def calc_softmax(a):\n",
        "    #return np.exp(a) / np.sum(np.exp(a), axis=0)\n",
        "    exp_a = np.exp(a - np.max(a, axis=0))\n",
        "    return exp_a / np.sum(exp_a, axis=0)\n",
        "\n",
        "# Forward propagation\n",
        "def forward_propagation(theta, inp_list, activation_func,no_of_hidden_layers):\n",
        "    a_h_list = []\n",
        "    h = inp_list\n",
        "    for i in range(no_of_hidden_layers):\n",
        "        a = np.dot(theta[i], h) + theta[no_of_hidden_layers + 1 + i]\n",
        "        a_h_list.append(a)\n",
        "        h = calc_activation(a, activation_func)\n",
        "        a_h_list.append(h)\n",
        "    a = np.dot(theta[no_of_hidden_layers], h) + theta[-1]\n",
        "    a_h_list.append(a)\n",
        "    y_hat = calc_softmax(a)\n",
        "    a_h_list.append(y_hat)\n",
        "    return a_h_list\n",
        "\n",
        "# Calculate loss\n",
        "def calc_loss(yhat, actual, loss_type):\n",
        "\n",
        "    if loss_type == \"mean_squared_error\":\n",
        "        sum=0\n",
        "        for i in range(10):\n",
        "          if(i==actual):\n",
        "            sum+=(1-yhat[i][0])**2\n",
        "          else:\n",
        "            sum+=yhat[i][0]**2\n",
        "        return  sum/10\n",
        "\n",
        "    elif loss_type == \"cross_entropy\":\n",
        "        prediction=yhat[actual][0]\n",
        "        if(not prediction):\n",
        "          prediction=1e-10\n",
        "        return -np.log(prediction)\n",
        "\n",
        "def calc_loss_acc(theta,validation_split,activation_func,loss_type,no_of_hidden_layers,which_loss,alpha):\n",
        "      correct = 0\n",
        "      total = int(60000 * validation_split)\n",
        "      loss = 0.0\n",
        "      if(which_loss==\"validation\"):\n",
        "        for i in range(59999, 59999 - total - 1, -1):\n",
        "            a_h_list = forward_propagation(theta, x_train[i].flatten().reshape((784, 1)), activation_func,no_of_hidden_layers)\n",
        "            prediction = np.argmax(a_h_list[-1])\n",
        "            if prediction == y_train[i]:\n",
        "                correct += 1\n",
        "            loss += calc_loss(a_h_list[-1], y_train[i], loss_type)\n",
        "        sumW = sum([np.sum(theta[i]**2) for i in range(no_of_hidden_layers+1)])\n",
        "        regularization_term = (alpha / 2) * sumW\n",
        "        accuracy = correct / total\n",
        "        loss = (loss + regularization_term) / total\n",
        "        return accuracy,loss\n",
        "      elif(which_loss==\"train\"):\n",
        "        for i in range(0, 60000-total):\n",
        "            a_h_list = forward_propagation(theta, x_train[i].flatten().reshape((784, 1)), activation_func,no_of_hidden_layers)\n",
        "            prediction = np.argmax(a_h_list[-1])\n",
        "            if prediction == y_train[i]:\n",
        "                correct += 1\n",
        "            loss += calc_loss(a_h_list[-1], y_train[i], loss_type)\n",
        "        sumW=sum([np.sum(theta[i]**2) for i in range(no_of_hidden_layers+1)])\n",
        "        regularization_term = (alpha / 2) * sumW\n",
        "        accuracy = correct / (60000-total)\n",
        "        loss = (loss + regularization_term) / (60000-total)\n",
        "        return accuracy,loss\n",
        "      elif(which_loss==\"test\"):\n",
        "        classes=[\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\", \"Sneaker\",\"Bag\", \"Ankle boot\"]\n",
        "        y_true=[]\n",
        "        y_pred=[]\n",
        "        for i in range(10000):\n",
        "          a_h_list = forward_propagation(theta, x_test[i].flatten().reshape((784, 1)), activation_func,no_of_hidden_layers)\n",
        "          prediction = np.argmax(a_h_list[-1])\n",
        "          if prediction == y_test[i]:\n",
        "              correct += 1\n",
        "          loss += calc_loss(a_h_list[-1], y_test[i], loss_type)\n",
        "          y_true.append(classes[y_test[i]])\n",
        "          y_pred.append(classes[prediction])\n",
        "        accuracy = correct /10000\n",
        "        loss = loss / 10000\n",
        "        return accuracy,loss,y_true,y_pred\n",
        "\n",
        "# Back propagation\n",
        "def back_propagation(a_h_list, y, inp, del_theta, theta, batch_size, activation_func,no_of_hidden_layers):\n",
        "    h_counter = len(a_h_list) - 1\n",
        "    grad_a = calc_aL(a_h_list[h_counter],y)\n",
        "    h_counter -= 2\n",
        "    for i in range(no_of_hidden_layers, -1, -1):\n",
        "        if i == 0:\n",
        "            del_w = np.dot(grad_a, inp.T)\n",
        "            del_b = grad_a\n",
        "            del_theta[i] = np.add(del_theta[i], del_w)\n",
        "            del_theta[i + no_of_hidden_layers + 1] = np.add(del_theta[i + no_of_hidden_layers + 1], del_b)\n",
        "            break\n",
        "        del_w = np.dot(grad_a, a_h_list[h_counter].T)\n",
        "        del_b = grad_a\n",
        "        del_theta[i] = np.add(del_theta[i], del_w)\n",
        "        del_theta[i + no_of_hidden_layers + 1] = np.add(del_theta[i + no_of_hidden_layers + 1], del_b)\n",
        "        grad_h_prev = np.dot(theta[i].T, grad_a)\n",
        "        grad_a = grad_h_prev * calc_gdash(a_h_list[h_counter - 1], activation_func)\n",
        "        h_counter -= 2\n",
        "\n",
        "# Gradient Descent\n",
        "def gradient_descent(eta, batch_size, epoch, theta, activation_func, validation_split, loss_type, alpha,no_of_hidden_layers):\n",
        "    for itr in range(epoch):\n",
        "        # Initialize gradients and total loss\n",
        "        del_theta = [np.zeros_like(param) for param in theta]\n",
        "\n",
        "        # Iterate through the training data\n",
        "        for i in tqdm(range(int(60000 * (1 - validation_split)))):\n",
        "            # Forward propagation\n",
        "            a_h_list = forward_propagation(theta, np.float128(x_train[i].flatten().reshape((784, 1))), activation_func,no_of_hidden_layers)\n",
        "\n",
        "            # Backpropagation\n",
        "            back_propagation(a_h_list, y_train[i], np.float128((x_train[i].flatten()).reshape((784, 1))), del_theta, theta, batch_size, activation_func,no_of_hidden_layers)\n",
        "\n",
        "            # Update weights after every mini-batch\n",
        "            if i % batch_size == 0 and i != 0:\n",
        "                for j in range(len(theta)):\n",
        "                    if(j<=no_of_hidden_layers):\n",
        "                      del_theta[j] = (del_theta[j] / batch_size)\n",
        "                    theta[j] = np.subtract(theta[j], eta * del_theta[j])-eta*alpha*theta[j]\n",
        "                    del_theta[j] = del_theta[j] * 0\n",
        "\n",
        "        # Calculate loss and accuracy\n",
        "        train_accuracy,train_loss=calc_loss_acc(theta,validation_split,activation_func,loss_type,no_of_hidden_layers,\"train\",alpha)\n",
        "        validation_accuracy,validation_loss=calc_loss_acc(theta,validation_split,activation_func,loss_type,no_of_hidden_layers,\"validation\",alpha)\n",
        "\n",
        "        # Print epoch statistics\n",
        "        wandb.log({'epoch':itr+1,\n",
        "          'loss': train_loss ,\n",
        "          'accuracy': train_accuracy,\n",
        "          'val_loss': validation_loss,\n",
        "          'val_accuracy': validation_accuracy\n",
        "      })\n",
        "\n",
        "\n",
        "# Momentum Gradient Descent\n",
        "def momentum_gradient_descent(eta, batch_size, epoch, theta, beta, activation_func, validation_split, loss_type, alpha,no_of_hidden_layers):\n",
        "    # Initialize previous history\n",
        "    prev_history = [np.zeros_like(param) for param in theta]\n",
        "\n",
        "    for itr in range(epoch):\n",
        "        del_theta = [np.zeros_like(param) for param in theta]\n",
        "\n",
        "        # Iterate through the training data\n",
        "        for i in tqdm(range(int(60000 * (1 - validation_split)))):\n",
        "            # Forward propagation\n",
        "            a_h_list = forward_propagation(theta, np.float128(x_train[i].flatten().reshape((784, 1))), activation_func,no_of_hidden_layers)\n",
        "\n",
        "            # Backpropagation\n",
        "            back_propagation(a_h_list, y_train[i], np.float128((x_train[i].flatten()).reshape((784, 1))), del_theta, theta, batch_size, activation_func,no_of_hidden_layers)\n",
        "\n",
        "            # Update weights using momentum\n",
        "            if i % batch_size == 0 and i != 0:\n",
        "                for j in range(len(del_theta)):\n",
        "                    if(j<=no_of_hidden_layers):\n",
        "                      del_theta[j] = (del_theta[j] / batch_size)\n",
        "                    prev_history[j] = np.add(beta * prev_history[j],eta * (del_theta[j]))\n",
        "                    theta[j] = np.subtract(theta[j], eta * prev_history[j])-eta*alpha*theta[j]\n",
        "                    del_theta[j] = del_theta[j] * 0\n",
        "\n",
        "        # Calculate loss and accuracy\n",
        "        train_accuracy,train_loss=calc_loss_acc(theta,validation_split,activation_func,loss_type,no_of_hidden_layers,\"train\",alpha)\n",
        "        validation_accuracy,validation_loss=calc_loss_acc(theta,validation_split,activation_func,loss_type,no_of_hidden_layers,\"validation\",alpha)\n",
        "\n",
        "        # Print epoch statistics\n",
        "        wandb.log({'epoch':itr+1,\n",
        "          'loss': train_loss ,\n",
        "          'accuracy': train_accuracy,\n",
        "          'val_loss': validation_loss,\n",
        "          'val_accuracy': validation_accuracy\n",
        "      })\n",
        "\n",
        "# Nestrov Gradient Descent\n",
        "def nesterov_gradient_descent(eta, batch_size, epoch, theta, beta, activation_func, validation_split, loss_type, alpha,no_of_hidden_layers):\n",
        "    # Initialize previous history\n",
        "    prev_history = [np.zeros_like(param) for param in theta]\n",
        "\n",
        "    for itr in range(epoch):\n",
        "        del_theta = [np.zeros_like(param) for param in theta]\n",
        "\n",
        "        # Iterate through the training data\n",
        "        for i in tqdm(range(int(60000 * (1 - validation_split)))):\n",
        "            # Update weights using Nesterov accelerated gradient descent\n",
        "            updated_theta = [theta[j] - beta * prev_history[j] for j in range(len(theta))]\n",
        "\n",
        "            # Forward propagation\n",
        "            a_h_list = forward_propagation(updated_theta, np.float128(x_train[i].flatten().reshape((784, 1))), activation_func,no_of_hidden_layers)\n",
        "\n",
        "            # Backpropagation\n",
        "            back_propagation(a_h_list, y_train[i], np.float128((x_train[i].flatten()).reshape((784, 1))), del_theta, updated_theta, batch_size, activation_func,no_of_hidden_layers)\n",
        "\n",
        "            # Update weights using momentum\n",
        "            if i % batch_size == 0 and i != 0:\n",
        "                for j in range(len(del_theta)):\n",
        "                    if(j<=no_of_hidden_layers):\n",
        "                      del_theta[j] = (del_theta[j] / batch_size)\n",
        "                    prev_history[j] = np.add(beta * prev_history[j],eta*(del_theta[j]))\n",
        "                    theta[j] = np.subtract(theta[j], prev_history[j])-eta*alpha*theta[j]\n",
        "                    del_theta[j] = del_theta[j] * 0\n",
        "\n",
        "        # Calculate loss and accuracy\n",
        "        train_accuracy,train_loss=calc_loss_acc(theta,validation_split,activation_func,loss_type,no_of_hidden_layers,\"train\",alpha)\n",
        "        validation_accuracy,validation_loss=calc_loss_acc(theta,validation_split,activation_func,loss_type,no_of_hidden_layers,\"validation\",alpha)\n",
        "\n",
        "        # Print epoch statistics\n",
        "        wandb.log({'epoch':itr+1,\n",
        "          'loss': train_loss ,\n",
        "          'accuracy': train_accuracy,\n",
        "          'val_loss': validation_loss,\n",
        "          'val_accuracy': validation_accuracy\n",
        "      })\n",
        "\n",
        "# RMS_Prop\n",
        "def rmsprop(eta, batch_size, epoch, theta, beta, eps, activation_func, validation_split, loss_type, alpha,no_of_hidden_layers):\n",
        "    # Initialize first  moment estimates\n",
        "    v_theta = [np.zeros_like(param) for param in theta]\n",
        "\n",
        "    for itr in range(epoch):\n",
        "        del_theta = [np.zeros_like(param) for param in theta]\n",
        "\n",
        "        # Iterate through the training data\n",
        "        for i in tqdm(range(int(60000 * (1 - validation_split)))):\n",
        "            # Forward propagation\n",
        "            a_h_list = forward_propagation(theta, np.float128(x_train[i].flatten().reshape((784, 1))), activation_func,no_of_hidden_layers)\n",
        "\n",
        "            # Backpropagation\n",
        "            back_propagation(a_h_list, y_train[i], np.float128((x_train[i].flatten()).reshape((784, 1))), del_theta, theta, batch_size, activation_func,no_of_hidden_layers)\n",
        "\n",
        "            # Update weights using Adam optimizer\n",
        "            if i % batch_size == 0 and i != 0:\n",
        "                for j in range(len(theta)):\n",
        "                    if(j<=no_of_hidden_layers):\n",
        "                      del_theta[j] = (del_theta[j] / batch_size)\n",
        "                    v_theta[j] = beta * v_theta[j] + (1 - beta) * (del_theta[j] ** 2)\n",
        "\n",
        "                    # Update weights\n",
        "                    theta[j] = np.subtract(theta[j], eta * (del_theta[j]/ (np.sqrt(v_theta[j] + eps))))-eta*alpha*theta[j]\n",
        "                    del_theta[j] = del_theta[j] * 0\n",
        "\n",
        "        # Calculate loss and accuracy\n",
        "        train_accuracy,train_loss=calc_loss_acc(theta,validation_split,activation_func,loss_type,no_of_hidden_layers,\"train\",alpha)\n",
        "        validation_accuracy,validation_loss=calc_loss_acc(theta,validation_split,activation_func,loss_type,no_of_hidden_layers,\"validation\",alpha)\n",
        "\n",
        "        # Print epoch statistics\n",
        "        wandb.log({'epoch':itr+1,\n",
        "          'loss': train_loss ,\n",
        "          'accuracy': train_accuracy,\n",
        "          'val_loss': validation_loss,\n",
        "          'val_accuracy': validation_accuracy\n",
        "      })\n",
        "\n",
        "\n",
        "# Adam Optimizer\n",
        "def adam_optimizer(eta, batch_size, epoch, theta, beta1, beta2, eps, activation_func, validation_split, loss_type, alpha,no_of_hidden_layers):\n",
        "    # Initialize first and second moment estimates\n",
        "    m_theta = [np.zeros_like(param) for param in theta]\n",
        "    v_theta = [np.zeros_like(param) for param in theta]\n",
        "\n",
        "    for itr in range(epoch):\n",
        "        del_theta = [np.zeros_like(param) for param in theta]\n",
        "\n",
        "        # Iterate through the training data\n",
        "        for i in tqdm(range(int(60000 * (1 - validation_split)))):\n",
        "            # Forward propagation\n",
        "            a_h_list = forward_propagation(theta, np.float128(x_train[i].flatten().reshape((784, 1))), activation_func,no_of_hidden_layers)\n",
        "\n",
        "            # Backpropagation\n",
        "            back_propagation(a_h_list, y_train[i], np.float128((x_train[i].flatten()).reshape((784, 1))), del_theta, theta, batch_size, activation_func,no_of_hidden_layers)\n",
        "\n",
        "            # Update weights using Adam optimizer\n",
        "            if i % batch_size == 0 and i != 0:\n",
        "                for j in range(len(theta)):\n",
        "                    if(j<=no_of_hidden_layers):\n",
        "                      del_theta[j] = (del_theta[j] / batch_size)\n",
        "                    m_theta[j] = beta1 * m_theta[j] + (1 - beta1) * del_theta[j]\n",
        "                    v_theta[j] = beta2 * v_theta[j] + (1 - beta2) * (del_theta[j] ** 2)\n",
        "\n",
        "                    # Bias correction\n",
        "                    m_hat = m_theta[j] / (1 - np.power(beta1, itr + 1))\n",
        "                    v_hat = v_theta[j] / (1 - np.power(beta2, itr + 1))\n",
        "\n",
        "                    # Update weights\n",
        "                    theta[j] = np.subtract(theta[j], eta * m_hat / (np.sqrt(v_hat) + eps))-eta*alpha*theta[j]\n",
        "                    del_theta[j] = del_theta[j] * 0\n",
        "\n",
        "         # Calculate loss and accuracy\n",
        "        train_accuracy,train_loss=calc_loss_acc(theta,validation_split,activation_func,loss_type,no_of_hidden_layers,\"train\",alpha)\n",
        "        validation_accuracy,validation_loss=calc_loss_acc(theta,validation_split,activation_func,loss_type,no_of_hidden_layers,\"validation\",alpha)\n",
        "\n",
        "        # Print epoch statistics\n",
        "        wandb.log({'epoch':itr+1,\n",
        "          'loss': train_loss ,\n",
        "          'accuracy': train_accuracy,\n",
        "          'val_loss': validation_loss,\n",
        "          'val_accuracy': validation_accuracy\n",
        "      })\n",
        "\n",
        "# nadam Optimizer\n",
        "def nadam_optimizer(eta, batch_size, epoch, theta, beta1, beta2, eps, activation_func, validation_split, loss_type, alpha ,no_of_hidden_layers):\n",
        "    # Initialize first and second moment estimates\n",
        "    m_theta = [np.zeros_like(param) for param in theta]\n",
        "    v_theta = [np.zeros_like(param) for param in theta]\n",
        "\n",
        "    for itr in range(epoch):\n",
        "        del_theta = [np.zeros_like(param) for param in theta]\n",
        "\n",
        "        # Iterate through the training data\n",
        "        for i in tqdm(range(int(60000 * (1 - validation_split)))):\n",
        "            # Forward propagation\n",
        "            a_h_list = forward_propagation(theta, np.float128(x_train[i].flatten().reshape((784, 1))), activation_func,no_of_hidden_layers)\n",
        "\n",
        "            # Backpropagation\n",
        "            back_propagation(a_h_list, y_train[i], np.float128((x_train[i].flatten()).reshape((784, 1))), del_theta, theta, batch_size, activation_func,no_of_hidden_layers)\n",
        "\n",
        "            # Update weights using Adam optimizer\n",
        "            if i % batch_size == 0 and i != 0:\n",
        "                for j in range(len(theta)):\n",
        "                    if(j<=no_of_hidden_layers):\n",
        "                      del_theta[j] = (del_theta[j] / batch_size)\n",
        "                    m_theta[j] = beta1 * m_theta[j] + (1 - beta1) * del_theta[j]\n",
        "                    v_theta[j] = beta2 * v_theta[j] + (1 - beta2) * (del_theta[j] ** 2)\n",
        "\n",
        "                    # Bias correction\n",
        "                    m_hat = m_theta[j] / (1 - np.power(beta1, itr + 1))\n",
        "                    v_hat = v_theta[j] / (1 - np.power(beta2, itr + 1))\n",
        "\n",
        "                    # Update weights\n",
        "                    theta[j] = np.subtract(theta[j], (eta / (np.sqrt(v_hat) + eps))*((beta1*m_hat)+((1-beta1)*(del_theta[j]/(1 - np.power(beta1, itr + 1))))))-eta*alpha*theta[j]\n",
        "                    del_theta[j] = del_theta[j] * 0\n",
        "\n",
        "         # Calculate loss and accuracy\n",
        "        train_accuracy,train_loss=calc_loss_acc(theta,validation_split,activation_func,loss_type,no_of_hidden_layers,\"train\",alpha)\n",
        "        validation_accuracy,validation_loss=calc_loss_acc(theta,validation_split,activation_func,loss_type,no_of_hidden_layers,\"validation\",alpha)\n",
        "\n",
        "        # Print epoch statistics\n",
        "        wandb.log({'epoch':itr+1,\n",
        "          'loss': train_loss ,\n",
        "          'accuracy': train_accuracy,\n",
        "          'val_loss': validation_loss,\n",
        "          'val_accuracy': validation_accuracy\n",
        "      })\n",
        "\n",
        "def plotConfusionMatrix(theta,activation_func,loss_type,no_of_hidden_layers,which_loss,alpha=0):\n",
        "    classes=[\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\", \"Sneaker\",\"Bag\", \"Ankle boot\"]\n",
        "    accuracy,loss,y_true,y_pred=calc_loss_acc(theta,0,activation_func,loss_type,no_of_hidden_layers,which_loss,0)\n",
        "    print(accuracy,loss)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            plt.text(j, i, format(cm[i, j], 'd'),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    wandb.log({\"confusion_matrix\": plt})\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def run_optimizer(eta, batch_size, epoch, theta, beta, beta1, beta2, eps, activation_func, validation_split, loss_type, alpha,optimizer,no_of_hidden_layers):\n",
        "    if(optimizer==\"sgd\"):\n",
        "      gradient_descent(eta,1,epoch,theta,activation_func,validation_split,loss_type,alpha,no_of_hidden_layers)\n",
        "    elif(optimizer==\"momentum\"):\n",
        "      momentum_gradient_descent(eta,batch_size,epoch,theta,beta,activation_func,validation_split,loss_type,alpha,no_of_hidden_layers)\n",
        "    elif(optimizer==\"nestrov\"):\n",
        "      nesterov_gradient_descent(eta,batch_size,epoch,theta,beta,activation_func,validation_split,loss_type,alpha,no_of_hidden_layers)\n",
        "    elif(optimizer==\"rmsprop\"):\n",
        "      rmsprop(eta,batch_size,epoch,theta,beta,eps,activation_func,validation_split,loss_type,alpha,no_of_hidden_layers)\n",
        "    elif(optimizer==\"adam\"):\n",
        "      adam_optimizer(eta,batch_size,epoch,theta,beta1,beta2,eps,activation_func,validation_split,loss_type,alpha,no_of_hidden_layers)\n",
        "    elif(optimizer==\"nadam\"):\n",
        "      nadam_optimizer(eta,batch_size,epoch,theta,beta1,beta2,eps,activation_func,validation_split,loss_type,alpha,no_of_hidden_layers)\n",
        "\n",
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'metric': {\n",
        "        'name': 'val_accuracy',\n",
        "        'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'learning_rate': {'values': [1e-3]},\n",
        "        'batch_size': {'values': [64]},\n",
        "        'num_layers':{'values':[4]},\n",
        "        'hidden_size':{'values':[128]},\n",
        "        'epochs': {'values': [5]},\n",
        "        'weight_decay': {'values': [0.0005]},\n",
        "        'optimizer': {'values': ['adam']},\n",
        "        'loss':{'values':['cross_entropy']},\n",
        "        'activation':{'values':['ReLU']},\n",
        "        'weight_init':{'values':['Xavier']}\n",
        "    }\n",
        "}\n",
        "\n",
        "def train():\n",
        "    # Initialize wandb\n",
        "    wandb.init()\n",
        "    # Set your hyperparameters from wandb config\n",
        "    config = wandb.config\n",
        "    wandb.run.name = f'hl_{config.num_layers}_bs_{config.batch_size}_ac_{config.activation}'\n",
        "\n",
        "    theta = initialize_weights(config.weight_init, 784, config.num_layers, 10, config.hidden_size)\n",
        "    # Train your model\n",
        "    run_optimizer(config.learning_rate, config.batch_size, config.epochs, theta, 0.9, 0.9, 0.999, 1e-10, config.activation, 0.1, config.loss, config.weight_decay, config.optimizer, config.num_layers)\n",
        "\n",
        "    #plot confusion matrix on best model\n",
        "    #plotConfusionMatrix(theta,config.activation,config.loss,config.num_layers,\"test\")\n",
        "# Initialize wandb sweep\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"deep_learn_assignment_1\")\n",
        "\n",
        "# Run wandb agent to execute the sweep\n",
        "wandb.agent(sweep_id, function=train, count =1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkjsPSe4DRB-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpIzU2O+K19D8ei2Jq9Oso"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
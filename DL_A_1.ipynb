{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.datasets import fashion_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load Fashion MNIST dataset\n",
        "def load_fashion_mnist():\n",
        "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "    x_train = x_train / 255.0\n",
        "    x_test = x_test / 255.0\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "# Display sample images\n",
        "def display_sample_images(x_train, y_train, label):\n",
        "    print_Once = [1] * 10\n",
        "    count = 10\n",
        "    for i in range(60000):\n",
        "        if count == 0:\n",
        "            break\n",
        "        if print_Once[y_train[i]]:\n",
        "            print_Once[y_train[i]] -= 1\n",
        "            count -= 1\n",
        "            plt.figure(figsize=(1, 1))\n",
        "            plt.imshow(x_train[i], cmap=\"Greys\")\n",
        "            plt.title(\"{}\".format(label[y_train[i]]))\n",
        "            plt.xticks([])\n",
        "            plt.yticks([])\n",
        "            plt.show()\n",
        "\n",
        "# Initialize weights and biases\n",
        "def initialize_weights(initialization_func, prev_layer_neurons, no_of_hidden_layers, classes):\n",
        "    theta = []\n",
        "    for i in range(2 * (no_of_hidden_layers + 1)):\n",
        "        theta.append([])\n",
        "    for i in range(no_of_hidden_layers):\n",
        "        neurons_in_layer = int(input(f\"Number of neurons in layer {i}: \"))\n",
        "        make_weights(theta, neurons_in_layer, prev_layer_neurons, i, initialization_func)\n",
        "        make_biases(theta, neurons_in_layer, no_of_hidden_layers + 1 + i, initialization_func)\n",
        "        prev_layer_neurons = neurons_in_layer\n",
        "    make_weights(theta, classes, prev_layer_neurons, no_of_hidden_layers, initialization_func)\n",
        "    make_biases(theta, classes, 2 * (no_of_hidden_layers + 1) - 1, initialization_func)\n",
        "    return theta\n",
        "\n",
        "# Make weights\n",
        "def make_weights(theta, curr_layer_neurons, prev_layer_neurons, layer_no, initialization_func):\n",
        "    if initialization_func == \"random\":\n",
        "        theta[layer_no] = np.float128(np.random.randn(curr_layer_neurons, prev_layer_neurons) * 0.01)\n",
        "    elif initialization_func == \"Xavier\":\n",
        "        factor = np.sqrt(6.0 / (curr_layer_neurons + prev_layer_neurons))\n",
        "        theta[layer_no] = np.float128(np.random.uniform(low=-factor, high=factor, size=(curr_layer_neurons, prev_layer_neurons)))\n",
        "\n",
        "# Make biases\n",
        "def make_biases(theta, curr_layer_neurons, layer_no, initialization_func):\n",
        "    if initialization_func == \"random\":\n",
        "        theta[layer_no] = np.float128(np.random.randn(curr_layer_neurons, 1) * 0.01)\n",
        "    elif initialization_func == \"Xavier\":\n",
        "        theta[layer_no] = np.float128(np.zeros((curr_layer_neurons, 1)))\n",
        "\n",
        "# Calculate activation function\n",
        "def calc_activation(a, activation_func):\n",
        "    h = []\n",
        "    a=np.round(a,6)\n",
        "    for i in range(len(a)):\n",
        "        if activation_func == \"sigmoid\":\n",
        "          if(a[i][0]<-100):\n",
        "            h.append(0.0)\n",
        "          else:\n",
        "            h.append(1/(1+np.exp(-a[i][0])))\n",
        "\n",
        "        elif activation_func == \"ReLU\":\n",
        "            h.append(max(0, a[i][0]))\n",
        "\n",
        "        elif activation_func == \"tanh\":\n",
        "           h.append(2 * (1 / (1 + np.exp(-2 * a[i][0]))) - 1)\n",
        "    h = np.array(h)\n",
        "    h_new = h.reshape((len(h), 1))\n",
        "    return h_new\n",
        "\n",
        "# Calculate activation derivative\n",
        "def calc_activation_derivative(a, activation_func):\n",
        "    if activation_func == \"sigmoid\":\n",
        "        a=0.0 if(a<-100) else 1/(1+np.exp(a))\n",
        "        return a * (1 - (a))\n",
        "    elif activation_func == \"ReLU\":\n",
        "        return np.where(a > 0, 1, 0)\n",
        "    elif activation_func == \"tanh\":\n",
        "        return 1/(np.cosh(a)**2)\n",
        "\n",
        "def calc_gdash(ak,activationFunc):\n",
        "  gdsh=[]\n",
        "  for i in ak:\n",
        "      gdsh.append(calc_activation_derivative(i[0],activationFunc))\n",
        "  gdsh=np.array(gdsh)\n",
        "  gdshNew=gdsh.reshape((len(gdsh),1))\n",
        "  return gdshNew\n",
        "\n",
        "def calc_aL(aL,y):\n",
        "  aL[y][0]=-(1-aL[y][0])\n",
        "  return aL\n",
        "\n",
        "# Calculate softmax\n",
        "def calc_softmax(a):\n",
        "    #return np.exp(a) / np.sum(np.exp(a), axis=0)\n",
        "    exp_a = np.exp(a - np.max(a, axis=0))\n",
        "    return exp_a / np.sum(exp_a, axis=0)\n",
        "\n",
        "# Forward propagation\n",
        "def forward_propagation(theta, inp_list, activation_func):\n",
        "    a_h_list = []\n",
        "    h = inp_list\n",
        "    for i in range(no_of_hidden_layers):\n",
        "        a = np.dot(theta[i], h) + theta[no_of_hidden_layers + 1 + i]\n",
        "        a_h_list.append(a)\n",
        "        h = calc_activation(a, activation_func)\n",
        "        a_h_list.append(h)\n",
        "    a = np.dot(theta[no_of_hidden_layers], h) + theta[-1]\n",
        "    a_h_list.append(a)\n",
        "    y_hat = calc_softmax(a)\n",
        "    a_h_list.append(y_hat)\n",
        "    return a_h_list\n",
        "\n",
        "# Calculate loss\n",
        "def calc_loss(prediction, actual, loss_type):\n",
        "    if loss_type == \"mse\":\n",
        "        return 0.5 * np.mean(np.square(prediction - actual))\n",
        "    elif loss_type == \"cross_entropy\":\n",
        "        prediction = np.clip(prediction, 1e-10, 1.0 - 1e-10)\n",
        "        return -np.log(prediction)\n",
        "\n",
        "def calc_val_loss_acc(theta,validation_split,activation_func,loss_type):\n",
        "      correct = 0\n",
        "      total = int(60000 * validation_split)\n",
        "      validation_loss = 0.0\n",
        "      for i in range(59999, 59999 - total - 1, -1):\n",
        "          a_h_list = forward_propagation(theta, x_train[i].flatten().reshape((784, 1)), activation_func)\n",
        "          prediction = np.argmax(a_h_list[-1])\n",
        "          if prediction == y_train[i]:\n",
        "              correct += 1\n",
        "          validation_loss += calc_loss(prediction, y_train[i], loss_type)\n",
        "      validation_accuracy = correct / total\n",
        "      validation_loss /= total\n",
        "      return validation_accuracy,validation_loss\n",
        "\n",
        "# Back propagation\n",
        "def back_propagation(a_h_list, y, inp, del_theta, theta, batch_size, activation_func):\n",
        "    h_counter = len(a_h_list) - 1\n",
        "    grad_a = calc_aL(a_h_list[h_counter],y)\n",
        "    h_counter -= 2\n",
        "    for i in range(no_of_hidden_layers, -1, -1):\n",
        "        if i == 0:\n",
        "            del_w = np.dot(grad_a, inp.T)\n",
        "            del_b = grad_a\n",
        "            del_theta[i] = np.add(del_theta[i], del_w)\n",
        "            del_theta[i + no_of_hidden_layers + 1] = np.add(del_theta[i + no_of_hidden_layers + 1], del_b)\n",
        "            break\n",
        "        del_w = np.dot(grad_a, a_h_list[h_counter].T)\n",
        "        del_b = grad_a\n",
        "        del_theta[i] = np.add(del_theta[i], del_w)\n",
        "        del_theta[i + no_of_hidden_layers + 1] = np.add(del_theta[i + no_of_hidden_layers + 1], del_b)\n",
        "        grad_h_prev = np.dot(theta[i].T, grad_a)\n",
        "        grad_a = grad_h_prev * calc_gdash(a_h_list[h_counter - 1], activation_func)\n",
        "        h_counter -= 2\n",
        "\n",
        "# Gradient Descent\n",
        "def gradient_descent(eta, batch_size, epoch, theta, activation_func, validation_split, loss_type, alpha):\n",
        "    for itr in range(epoch):\n",
        "        # Initialize gradients and total loss\n",
        "        del_theta = [np.zeros_like(param) for param in theta]\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "\n",
        "        # Iterate through the training data\n",
        "        for i in tqdm(range(int(60000 * (1 - validation_split)))):\n",
        "            # Forward propagation\n",
        "            a_h_list = forward_propagation(theta, np.float128(x_train[i].flatten().reshape((784, 1))), activation_func)\n",
        "\n",
        "            # Compute loss\n",
        "            total_loss += calc_loss(np.argmax(a_h_list[-1]), y_train[i], loss_type)\n",
        "\n",
        "            # Update accuracy\n",
        "            if np.argmax(a_h_list[-1]) == y_train[i]:\n",
        "                total_acc += 1\n",
        "\n",
        "            # Backpropagation\n",
        "            back_propagation(a_h_list, y_train[i], np.float128((x_train[i].flatten()).reshape((784, 1))), del_theta, theta, batch_size, activation_func)\n",
        "\n",
        "            # Update weights after every mini-batch\n",
        "            if i % batch_size == 0 and i != 0:\n",
        "                for j in range(len(theta)):\n",
        "                    del_theta[j] = (del_theta[j] / batch_size)+alpha*theta[j]\n",
        "                    #temp=np.subtract(theta[j],eta*del_theta[j])\n",
        "                    #norm=np.linalg.norm(temp)\n",
        "                    #theta[j]=temp/norm\n",
        "                    theta[j] = np.subtract(theta[j], eta * del_theta[j])\n",
        "                    del_theta[j] = del_theta[j] * 0\n",
        "\n",
        "        # Calculate validation loss and accuracy\n",
        "        validation_accuracy,validation_loss=calc_val_loss_acc(theta,validation_split,activation_func,loss_type)\n",
        "\n",
        "        # Print epoch statistics\n",
        "        print(f\"Epoch {itr + 1}/{epoch}, Loss: {total_loss / (60000 * (1 - validation_split))}, Accuracy: {total_acc / (60000 * (1 - validation_split))}, Validation Loss: {validation_loss}, Validation Accuracy: {validation_accuracy}\")\n",
        "\n",
        "# Momentum Gradient Descent\n",
        "def momentum_gradient_descent(eta, batch_size, epoch, theta, beta, activationFunc, validation_split, loss_type, alpha):\n",
        "    # Initialize previous history\n",
        "    prev_history = [np.zeros_like(param) for param in theta]\n",
        "\n",
        "    for itr in range(epoch):\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        del_theta = [np.zeros_like(param) for param in theta]\n",
        "\n",
        "        # Iterate through the training data\n",
        "        for i in tqdm(range(int(60000 * (1 - validation_split)))):\n",
        "            # Forward propagation\n",
        "            a_h_list = forward_propagation(theta, np.float128(x_train[i].flatten().reshape((784, 1))), activationFunc)\n",
        "\n",
        "            # Compute loss\n",
        "            total_loss += calc_loss(np.argmax(a_h_list[-1]), y_train[i], loss_type)\n",
        "\n",
        "            # Update accuracy\n",
        "            if np.argmax(a_h_list[-1]) == y_train[i]:\n",
        "                total_acc += 1\n",
        "\n",
        "            # Backpropagation\n",
        "            back_propagation(a_h_list, y_train[i], np.float128((x_train[i].flatten()).reshape((784, 1))), del_theta, theta, batch_size, activationFunc)\n",
        "\n",
        "            # Update weights using momentum\n",
        "            if i % batch_size == 0 and i != 0:\n",
        "                for j in range(len(del_theta)):\n",
        "                    del_theta[j] = (del_theta[j] / batch_size)+alpha*theta[j]\n",
        "                    prev_history[j] = np.add(beta * prev_history[j],eta * (del_theta[j]))\n",
        "                    theta[j] = np.subtract(theta[j], eta * prev_history[j])\n",
        "                    del_theta[j] = del_theta[j] * 0\n",
        "\n",
        "        # Calculate validation loss and accuracy\n",
        "        validation_accuracy,validation_loss=calc_val_loss_acc(theta,validation_split,activation_func,loss_type)\n",
        "\n",
        "        # Print epoch statistics\n",
        "        print(f\"Epoch {itr + 1}/{epoch}, Loss: {total_loss / (60000 * (1 - validation_split))}, Accuracy: {total_acc / (60000 * (1 - validation_split))}, Validation Loss: {validation_loss}, Validation Accuracy: {validation_accuracy}\")\n",
        "\n",
        "# Nestrov Gradient Descent\n",
        "def nesterov_gradient_descent(eta, batch_size, epoch, theta, beta, activationFunc, validation_split, loss_type, alpha):\n",
        "    # Initialize previous history\n",
        "    prev_history = [np.zeros_like(param) for param in theta]\n",
        "\n",
        "    for itr in range(epoch):\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        del_theta = [np.zeros_like(param) for param in theta]\n",
        "\n",
        "        # Iterate through the training data\n",
        "        for i in tqdm(range(int(60000 * (1 - validation_split)))):\n",
        "            # Update weights using Nesterov accelerated gradient descent\n",
        "            updated_theta = [theta[j] - beta * prev_history[j] for j in range(len(theta))]\n",
        "\n",
        "            # Forward propagation\n",
        "            a_h_list = forward_propagation(updated_theta, np.float128(x_train[i].flatten().reshape((784, 1))), activationFunc)\n",
        "\n",
        "            # Compute loss\n",
        "            total_loss += calc_loss(np.argmax(a_h_list[-1]), y_train[i], loss_type)\n",
        "\n",
        "            # Update accuracy\n",
        "            if np.argmax(a_h_list[-1]) == y_train[i]:\n",
        "                total_acc += 1\n",
        "\n",
        "            # Backpropagation\n",
        "            back_propagation(a_h_list, y_train[i], np.float128((x_train[i].flatten()).reshape((784, 1))), del_theta, updated_theta, batch_size, activationFunc)\n",
        "\n",
        "            # Update weights using momentum\n",
        "            if i % batch_size == 0 and i != 0:\n",
        "                for j in range(len(del_theta)):\n",
        "                    del_theta[j] = (del_theta[j] / batch_size)+alpha*theta[j]\n",
        "                    prev_history[j] = np.add(beta * prev_history[j],eta*(del_theta[j]))\n",
        "                    theta[j] = np.subtract(theta[j], prev_history[j])\n",
        "                    del_theta[j] = del_theta[j] * 0\n",
        "\n",
        "        # Calculate validation loss and accuracy\n",
        "        validation_accuracy,validation_loss=calc_val_loss_acc(theta,validation_split,activation_func,loss_type)\n",
        "\n",
        "        # Print epoch statistics\n",
        "        print(f\"Epoch {itr + 1}/{epoch}, Loss: {total_loss / (60000 * (1 - validation_split))}, Accuracy: {total_acc / (60000 * (1 - validation_split))}, Validation Loss: {validation_loss}, Validation Accuracy: {validation_accuracy}\")\n",
        "\n",
        "# RMS_Prop\n",
        "def rmsprop(eta, batch_size, epoch, theta, beta, eps, activation_func, validation_split, loss_type, alpha):\n",
        "    # Initialize first  moment estimates\n",
        "    v_theta = [np.zeros_like(param) for param in theta]\n",
        "\n",
        "    for itr in range(epoch):\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        del_theta = [np.zeros_like(param) for param in theta]\n",
        "\n",
        "        # Iterate through the training data\n",
        "        for i in tqdm(range(int(60000 * (1 - validation_split)))):\n",
        "            # Forward propagation\n",
        "            a_h_list = forward_propagation(theta, np.float128(x_train[i].flatten().reshape((784, 1))), activation_func)\n",
        "\n",
        "            # Compute loss\n",
        "            total_loss += calc_loss(np.argmax(a_h_list[-1]), y_train[i], loss_type)\n",
        "\n",
        "            # Update accuracy\n",
        "            if np.argmax(a_h_list[-1]) == y_train[i]:\n",
        "                total_acc += 1\n",
        "\n",
        "            # Backpropagation\n",
        "            back_propagation(a_h_list, y_train[i], np.float128((x_train[i].flatten()).reshape((784, 1))), del_theta, theta, batch_size, activation_func)\n",
        "\n",
        "            # Update weights using Adam optimizer\n",
        "            if i % batch_size == 0 and i != 0:\n",
        "                for j in range(len(theta)):\n",
        "                    del_theta[j] = (del_theta[j] / batch_size)+alpha*theta[j]\n",
        "                    v_theta[j] = beta * v_theta[j] + (1 - beta) * (del_theta[j] ** 2)\n",
        "\n",
        "                    # Update weights\n",
        "                    theta[j] = np.subtract(theta[j], eta * (del_theta[j]/ (np.sqrt(v_theta[j] + eps))))\n",
        "\n",
        "         # Calculate validation loss and accuracy\n",
        "        validation_accuracy,validation_loss=calc_val_loss_acc(theta,validation_split,activation_func,loss_type)\n",
        "\n",
        "        # Print epoch statistics\n",
        "        print(f\"Epoch {itr + 1}/{epoch}, Loss: {total_loss / (60000 * (1 - validation_split))}, Accuracy: {total_acc / (60000 * (1 - validation_split))}, Validation Loss: {validation_loss}, Validation Accuracy: {validation_accuracy}\")\n",
        "\n",
        "\n",
        "# Adam Optimizer\n",
        "def adam_optimizer(eta, batch_size, epoch, theta, beta1, beta2, eps, activation_func, validation_split, loss_type, alpha):\n",
        "    # Initialize first and second moment estimates\n",
        "    m_theta = [np.zeros_like(param) for param in theta]\n",
        "    v_theta = [np.zeros_like(param) for param in theta]\n",
        "\n",
        "    for itr in range(epoch):\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        del_theta = [np.zeros_like(param) for param in theta]\n",
        "\n",
        "        # Iterate through the training data\n",
        "        for i in tqdm(range(int(60000 * (1 - validation_split)))):\n",
        "            # Forward propagation\n",
        "            a_h_list = forward_propagation(theta, np.float128(x_train[i].flatten().reshape((784, 1))), activation_func)\n",
        "\n",
        "            # Compute loss\n",
        "            total_loss += calc_loss(np.argmax(a_h_list[-1]), y_train[i], loss_type)\n",
        "\n",
        "            # Update accuracy\n",
        "            if np.argmax(a_h_list[-1]) == y_train[i]:\n",
        "                total_acc += 1\n",
        "\n",
        "            # Backpropagation\n",
        "            back_propagation(a_h_list, y_train[i], np.float128((x_train[i].flatten()).reshape((784, 1))), del_theta, theta, batch_size, activation_func)\n",
        "\n",
        "            # Update weights using Adam optimizer\n",
        "            if i % batch_size == 0 and i != 0:\n",
        "                for j in range(len(theta)):\n",
        "                    del_theta[j] = (del_theta[j] / batch_size)+alpha*theta[j]\n",
        "                    m_theta[j] = beta1 * m_theta[j] + (1 - beta1) * del_theta[j]\n",
        "                    v_theta[j] = beta2 * v_theta[j] + (1 - beta2) * (del_theta[j] ** 2)\n",
        "\n",
        "                    # Bias correction\n",
        "                    m_hat = m_theta[j] / (1 - np.power(beta1, itr + 1))\n",
        "                    v_hat = v_theta[j] / (1 - np.power(beta2, itr + 1))\n",
        "\n",
        "                    # Update weights\n",
        "                    theta[j] = np.subtract(theta[j], eta * m_hat / (np.sqrt(v_hat) + eps))\n",
        "\n",
        "         # Calculate validation loss and accuracy\n",
        "        validation_accuracy,validation_loss=calc_val_loss_acc(theta,validation_split,activation_func,loss_type)\n",
        "\n",
        "        # Print epoch statistics\n",
        "        print(f\"Epoch {itr + 1}/{epoch}, Loss: {total_loss / (60000 * (1 - validation_split))}, Accuracy: {total_acc / (60000 * (1 - validation_split))}, Validation Loss: {validation_loss}, Validation Accuracy: {validation_accuracy}\")\n",
        "\n",
        "# nadam Optimizer\n",
        "def nadam_optimizer(eta, batch_size, epoch, theta, beta1, beta2, eps, activation_func, validation_split, loss_type, alpha):\n",
        "    # Initialize first and second moment estimates\n",
        "    m_theta = [np.zeros_like(param) for param in theta]\n",
        "    v_theta = [np.zeros_like(param) for param in theta]\n",
        "\n",
        "    for itr in range(epoch):\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        del_theta = [np.zeros_like(param) for param in theta]\n",
        "\n",
        "        # Iterate through the training data\n",
        "        for i in tqdm(range(int(60000 * (1 - validation_split)))):\n",
        "            # Forward propagation\n",
        "            a_h_list = forward_propagation(theta, np.float128(x_train[i].flatten().reshape((784, 1))), activation_func)\n",
        "\n",
        "            # Compute loss\n",
        "            total_loss += calc_loss(np.argmax(a_h_list[-1]), y_train[i], loss_type)\n",
        "\n",
        "            # Update accuracy\n",
        "            if np.argmax(a_h_list[-1]) == y_train[i]:\n",
        "                total_acc += 1\n",
        "\n",
        "            # Backpropagation\n",
        "            back_propagation(a_h_list, y_train[i], np.float128((x_train[i].flatten()).reshape((784, 1))), del_theta, theta, batch_size, activation_func)\n",
        "\n",
        "            # Update weights using Adam optimizer\n",
        "            if i % batch_size == 0 and i != 0:\n",
        "                for j in range(len(theta)):\n",
        "                    del_theta[j] = (del_theta[j] / batch_size)+alpha*theta[j]\n",
        "                    m_theta[j] = beta1 * m_theta[j] + (1 - beta1) * del_theta[j]\n",
        "                    v_theta[j] = beta2 * v_theta[j] + (1 - beta2) * (del_theta[j] ** 2)\n",
        "\n",
        "                    # Bias correction\n",
        "                    m_hat = m_theta[j] / (1 - np.power(beta1, itr + 1))\n",
        "                    v_hat = v_theta[j] / (1 - np.power(beta2, itr + 1))\n",
        "\n",
        "                    # Update weights\n",
        "                    theta[j] = np.subtract(theta[j], (eta / (np.sqrt(v_hat) + eps))*((beta1*m_hat)+((1-beta1)*(del_theta[j]/(1 - np.power(beta1, itr + 1))))))\n",
        "\n",
        "         # Calculate validation loss and accuracy\n",
        "        validation_accuracy,validation_loss=calc_val_loss_acc(theta,validation_split,activation_func,loss_type)\n",
        "\n",
        "        # Print epoch statistics\n",
        "        print(f\"Epoch {itr + 1}/{epoch}, Loss: {total_loss / (60000 * (1 - validation_split))}, Accuracy: {total_acc / (60000 * (1 - validation_split))}, Validation Loss: {validation_loss}, Validation Accuracy: {validation_accuracy}\")\n",
        "\n",
        "\n",
        "# Main code\n",
        "(x_train, y_train), (x_test, y_test) = load_fashion_mnist()\n",
        "label = {0: \"T-shirt/top\", 1: \"Trouser\", 2: \"Pullover\", 3: \"Dress\", 4: \"Coat\", 5: \"Sandal\", 6: \"Shirt\", 7: \"Sneaker\", 8: \"Bag\", 9: \"Ankle boot\"}\n",
        "no_of_hidden_layers = int(input(\"Enter the number of hidden layers: \"))\n",
        "theta = initialize_weights(\"random\", 784, no_of_hidden_layers, 10)\n",
        "eta = 1e-2\n",
        "batch_size = 32\n",
        "epoch = 5\n",
        "beta=0.9\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "eps = 1e-10\n",
        "alpha=0.0005\n",
        "validation_split = 0.1\n",
        "activation_func = \"sigmoid\"\n",
        "loss_type = \"cross_entropy\"\n",
        "gradient_descent(eta,1,epoch,theta,activation_func,validation_split,loss_type,alpha)\n",
        "#momentum_gradient_descent(eta,batch_size,epoch,theta,beta,activation_func,validation_split,loss_type,alpha)\n",
        "#nesterov_gradient_descent(eta,batch_size,epoch,theta,beta,activation_func,validation_split,loss_type,alpha)\n",
        "#rmsprop(eta,batch_size,epoch,theta,beta,eps,activation_func,validation_split,loss_type,alpha)\n",
        "#adam_optimizer(eta,batch_size,epoch,theta,beta1,beta2,eps,activation_func,validation_split,loss_type,alpha)\n",
        "#nadam_optimizer(eta,batch_size,epoch,theta,beta1,beta2,eps,activation_func,validation_split,loss_type,alpha)"
      ],
      "metadata": {
        "id": "5rM7z30DGsVH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9thRcMGP4hI13tT1TvU33"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
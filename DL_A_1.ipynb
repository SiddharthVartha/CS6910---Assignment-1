{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "_oG4bR3ld4vd",
        "outputId": "ea01f714-1fce-48a4-c22a-83f07c3d5257"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.4-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.42.0-py2.py3-none-any.whl (263 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.5/263.5 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.42 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.42.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6I010r3qgpm"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704,
          "referenced_widgets": [
            "f1803ba2038b4ac2a39c086bdc1189da",
            "f2357185363a458090f135efdba587a6",
            "d512c0f4c7f14d7fadce98aa444aa70e",
            "b6924ab39dec4e41af44b67b1ff221f4",
            "055cd1b30acc4491a247eb5475f44542",
            "42c5c55644634489bca6acd6a430fcdd",
            "77b25017855a4b4398832a44fb197918",
            "51269deaa4424633b7bac19cc5483c07"
          ]
        },
        "id": "5rM7z30DGsVH",
        "outputId": "7c5199ea-b706-4c28-94b3-ed10bd446b47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: 7ex4zh65\n",
            "Sweep URL: https://wandb.ai/deep_learn_assignment_1/deep_learn_assignment_1/sweeps/7ex4zh65\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: t6h9dc3f with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nestrov\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_init: random\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240316_190435-t6h9dc3f</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/deep_learn_assignment_1/deep_learn_assignment_1/runs/t6h9dc3f' target=\"_blank\">rosy-sweep-1</a></strong> to <a href='https://wandb.ai/deep_learn_assignment_1/deep_learn_assignment_1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/deep_learn_assignment_1/deep_learn_assignment_1/sweeps/7ex4zh65' target=\"_blank\">https://wandb.ai/deep_learn_assignment_1/deep_learn_assignment_1/sweeps/7ex4zh65</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/deep_learn_assignment_1/deep_learn_assignment_1' target=\"_blank\">https://wandb.ai/deep_learn_assignment_1/deep_learn_assignment_1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/deep_learn_assignment_1/deep_learn_assignment_1/sweeps/7ex4zh65' target=\"_blank\">https://wandb.ai/deep_learn_assignment_1/deep_learn_assignment_1/sweeps/7ex4zh65</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/deep_learn_assignment_1/deep_learn_assignment_1/runs/t6h9dc3f' target=\"_blank\">https://wandb.ai/deep_learn_assignment_1/deep_learn_assignment_1/runs/t6h9dc3f</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 54000/54000 [03:12<00:00, 281.12it/s]\n",
            "100%|██████████| 54000/54000 [03:08<00:00, 286.22it/s]\n",
            "100%|██████████| 54000/54000 [03:12<00:00, 280.89it/s]\n",
            "100%|██████████| 54000/54000 [03:13<00:00, 278.57it/s]\n",
            "100%|██████████| 54000/54000 [03:14<00:00, 278.03it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f1803ba2038b4ac2a39c086bdc1189da"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>█▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>loss</td><td>▁████</td></tr><tr><td>val_accuracy</td><td>█▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.09965</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>loss</td><td>2.30667</td></tr><tr><td>val_accuracy</td><td>0.10333</td></tr><tr><td>val_loss</td><td>2.3083</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">rosy-sweep-1</strong> at: <a href='https://wandb.ai/deep_learn_assignment_1/deep_learn_assignment_1/runs/t6h9dc3f' target=\"_blank\">https://wandb.ai/deep_learn_assignment_1/deep_learn_assignment_1/runs/t6h9dc3f</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240316_190435-t6h9dc3f/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.datasets import fashion_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "# Load Fashion MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "# Display sample images\n",
        "def display_sample_images():\n",
        "    label = {0: \"T-shirt/top\", 1: \"Trouser\", 2: \"Pullover\", 3: \"Dress\", 4: \"Coat\", 5: \"Sandal\", 6: \"Shirt\", 7: \"Sneaker\", 8: \"Bag\", 9: \"Ankle boot\"}\n",
        "    print_Once = [1] * 10\n",
        "    count = 10\n",
        "    fig, axes = plt.subplots(1, 10, figsize=(20, 4))\n",
        "\n",
        "    for i in range(60000):\n",
        "        if count == 0:\n",
        "            break\n",
        "        if print_Once[y_train[i]]:\n",
        "            print_Once[y_train[i]] -= 1\n",
        "            count -= 1\n",
        "            col = 10 - count\n",
        "            axes[col - 1].imshow(x_train[i], cmap='gray')\n",
        "            axes[col - 1].set_title(\"{}\".format(label[y_train[i]]))\n",
        "            axes[col - 1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    wandb.log({\"Sample Image\":plt})\n",
        "\n",
        "# Initialize weights and biases\n",
        "def initialize_weights(initialization_func, prev_layer_neurons, no_of_hidden_layers, classes,each_layer_neuron):\n",
        "    theta = []\n",
        "    for i in range(2 * (no_of_hidden_layers + 1)):\n",
        "        theta.append([])\n",
        "    for i in range(no_of_hidden_layers):\n",
        "        neurons_in_layer = each_layer_neuron\n",
        "        make_weights(theta, neurons_in_layer, prev_layer_neurons, i, initialization_func)\n",
        "        make_biases(theta, neurons_in_layer, no_of_hidden_layers + 1 + i, initialization_func)\n",
        "        prev_layer_neurons = neurons_in_layer\n",
        "    make_weights(theta, classes, prev_layer_neurons, no_of_hidden_layers, initialization_func)\n",
        "    make_biases(theta, classes, 2 * (no_of_hidden_layers + 1) - 1, initialization_func)\n",
        "    return theta\n",
        "\n",
        "# Make weights\n",
        "def make_weights(theta, curr_layer_neurons, prev_layer_neurons, layer_no, initialization_func):\n",
        "    if initialization_func == \"random\":\n",
        "        theta[layer_no] = np.float128(np.random.randn(curr_layer_neurons, prev_layer_neurons))\n",
        "    elif initialization_func == \"Xavier\":\n",
        "        factor = np.sqrt(6.0 / (curr_layer_neurons + prev_layer_neurons))\n",
        "        theta[layer_no] = np.float128(np.random.uniform(low=-factor, high=factor, size=(curr_layer_neurons, prev_layer_neurons)))\n",
        "\n",
        "# Make biases\n",
        "def make_biases(theta, curr_layer_neurons, layer_no, initialization_func):\n",
        "    if initialization_func == \"random\":\n",
        "        theta[layer_no] = np.float128(np.random.randn(curr_layer_neurons, 1))\n",
        "    elif initialization_func == \"Xavier\":\n",
        "        theta[layer_no] = np.float128(np.zeros((curr_layer_neurons, 1)))\n",
        "\n",
        "# Calculate activation function\n",
        "def calc_activation(a, activation_func):\n",
        "    h = []\n",
        "    a=np.round(a,6)\n",
        "    for i in range(len(a)):\n",
        "        if activation_func == \"sigmoid\":\n",
        "          if(a[i][0]<-30):\n",
        "            h.append(0.0)\n",
        "          else:\n",
        "            h.append(1/(1+np.exp(-a[i][0])))\n",
        "\n",
        "        elif activation_func == \"ReLU\":\n",
        "            h.append(max(0, a[i][0]))\n",
        "\n",
        "        elif activation_func == \"tanh\":\n",
        "           if(a[i][0]<-30):\n",
        "              h.append(-1.0)\n",
        "           else:\n",
        "              h.append(2 * (1 / (1 + np.exp(-2 * a[i][0]))) - 1)\n",
        "        elif activation_func==\"identity\":\n",
        "            h.append(a[i][0])\n",
        "    h = np.array(h)\n",
        "    h_new = h.reshape((len(h), 1))\n",
        "    return h_new\n",
        "\n",
        "# Calculate activation derivative\n",
        "def calc_activation_derivative(a, activation_func):\n",
        "    if activation_func == \"sigmoid\":\n",
        "        a=0.0 if(a<-30) else 1/(1+np.exp(a))\n",
        "        return a * (1 - (a))\n",
        "    elif activation_func == \"ReLU\":\n",
        "        return np.where(a > 0, 1, 0)\n",
        "    elif activation_func == \"tanh\":\n",
        "        return 1-(np.tanh(a)**2)\n",
        "    elif activation_func==\"identity\":\n",
        "        return 1\n",
        "\n",
        "def calc_gdash(ak,activationFunc):\n",
        "  gdsh=[]\n",
        "  for i in ak:\n",
        "      gdsh.append(calc_activation_derivative(i[0],activationFunc))\n",
        "  gdsh=np.array(gdsh)\n",
        "  gdshNew=gdsh.reshape((len(gdsh),1))\n",
        "  return gdshNew\n",
        "\n",
        "def calc_aL(aL,y,loss_type):\n",
        "  if(loss_type==\"cross_entropy\"):\n",
        "    aL[y][0]=-(1-aL[y][0])\n",
        "    return aL\n",
        "  elif(loss_type==\"mean_squared_error\"):\n",
        "    Y=np.zeros_like(aL)\n",
        "    Y[y][0]=1\n",
        "    print(Y,np.multiply(-2*(aL-Y),np.multiply(Y,(1-Y))))\n",
        "    return np.multiply(-2*(aL-Y),np.multiply(Y,(1-Y)))\n",
        "\n",
        "# Calculate softmax\n",
        "def calc_softmax(a):\n",
        "    #return np.exp(a) / np.sum(np.exp(a), axis=0)\n",
        "    exp_a = np.exp(a - np.max(a, axis=0))\n",
        "    return exp_a / np.sum(exp_a, axis=0)\n",
        "\n",
        "# Forward propagation\n",
        "def forward_propagation(theta, inp_list, activation_func,no_of_hidden_layers):\n",
        "    a_h_list = []\n",
        "    h = inp_list\n",
        "    for i in range(no_of_hidden_layers):\n",
        "        a = np.dot(theta[i], h) + theta[no_of_hidden_layers + 1 + i]\n",
        "        a_h_list.append(a)\n",
        "        h = calc_activation(a, activation_func)\n",
        "        a_h_list.append(h)\n",
        "    a = np.dot(theta[no_of_hidden_layers], h) + theta[-1]\n",
        "    a_h_list.append(a)\n",
        "    y_hat = calc_softmax(a)\n",
        "    a_h_list.append(y_hat)\n",
        "    return a_h_list\n",
        "\n",
        "# Calculate loss\n",
        "def calc_loss(yhat, actual, loss_type):\n",
        "\n",
        "    if loss_type == \"mean_squared_error\":\n",
        "        sum=0\n",
        "        for i in range(10):\n",
        "          if(i==actual):\n",
        "            sum+=(1-yhat[i][0])**2\n",
        "          else:\n",
        "            sum+=yhat[i][0]**2\n",
        "        return  sum/10\n",
        "\n",
        "    elif loss_type == \"cross_entropy\":\n",
        "        prediction=yhat[actual][0]\n",
        "        if(not prediction):\n",
        "          prediction=1e-10\n",
        "        return -np.log(prediction)\n",
        "\n",
        "def calc_loss_acc(theta,validation_split,activation_func,loss_type,no_of_hidden_layers,which_loss,alpha):\n",
        "      correct = 0\n",
        "      total = int(60000 * validation_split)\n",
        "      loss = 0.0\n",
        "      if(which_loss==\"validation\"):\n",
        "        for i in range(59999, 59999 - total - 1, -1):\n",
        "            a_h_list = forward_propagation(theta, x_train[i].flatten().reshape((784, 1)), activation_func,no_of_hidden_layers)\n",
        "            prediction = np.argmax(a_h_list[-1])\n",
        "            if prediction == y_train[i]:\n",
        "                correct += 1\n",
        "            loss += calc_loss(a_h_list[-1], y_train[i], loss_type)\n",
        "        sumW = sum([np.sum(theta[i]**2) for i in range(no_of_hidden_layers+1)])\n",
        "        regularization_term = (alpha / 2) * sumW\n",
        "        accuracy = correct / total\n",
        "        loss = (loss + regularization_term) / total\n",
        "        return accuracy,loss\n",
        "      elif(which_loss==\"train\"):\n",
        "        for i in range(0, 60000-total):\n",
        "            a_h_list = forward_propagation(theta, x_train[i].flatten().reshape((784, 1)), activation_func,no_of_hidden_layers)\n",
        "            prediction = np.argmax(a_h_list[-1])\n",
        "            if prediction == y_train[i]:\n",
        "                correct += 1\n",
        "            loss += calc_loss(a_h_list[-1], y_train[i], loss_type)\n",
        "        sumW=sum([np.sum(theta[i]**2) for i in range(no_of_hidden_layers+1)])\n",
        "        regularization_term = (alpha / 2) * sumW\n",
        "        accuracy = correct / (60000-total)\n",
        "        loss = (loss + regularization_term) / (60000-total)\n",
        "        return accuracy,loss\n",
        "      elif(which_loss==\"test\"):\n",
        "        classes=[\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\", \"Sneaker\",\"Bag\", \"Ankle boot\"]\n",
        "        y_true=[]\n",
        "        y_pred=[]\n",
        "        for i in range(10000):\n",
        "          a_h_list = forward_propagation(theta, x_test[i].flatten().reshape((784, 1)), activation_func,no_of_hidden_layers)\n",
        "          prediction = np.argmax(a_h_list[-1])\n",
        "          if prediction == y_test[i]:\n",
        "              correct += 1\n",
        "          loss += calc_loss(a_h_list[-1], y_test[i], loss_type)\n",
        "          y_true.append(classes[y_test[i]])\n",
        "          y_pred.append(classes[prediction])\n",
        "        accuracy = correct /10000\n",
        "        loss = loss / 10000\n",
        "        return accuracy,loss,y_true,y_pred\n",
        "\n",
        "# Back propagation\n",
        "def back_propagation(a_h_list, y, inp, del_theta, theta, batch_size, activation_func,no_of_hidden_layers,loss_type):\n",
        "    h_counter = len(a_h_list) - 1\n",
        "    grad_a = calc_aL(a_h_list[h_counter],y,loss_type)\n",
        "    h_counter -= 2\n",
        "    for i in range(no_of_hidden_layers, -1, -1):\n",
        "        if i == 0:\n",
        "            del_w = np.dot(grad_a, inp.T)\n",
        "            del_b = grad_a\n",
        "            del_theta[i] = np.add(del_theta[i], del_w)\n",
        "            del_theta[i + no_of_hidden_layers + 1] = np.add(del_theta[i + no_of_hidden_layers + 1], del_b)\n",
        "            break\n",
        "        del_w = np.dot(grad_a, a_h_list[h_counter].T)\n",
        "        del_b = grad_a\n",
        "        del_theta[i] = np.add(del_theta[i], del_w)\n",
        "        del_theta[i + no_of_hidden_layers + 1] = np.add(del_theta[i + no_of_hidden_layers + 1], del_b)\n",
        "        grad_h_prev = np.dot(theta[i].T, grad_a)\n",
        "        grad_a = grad_h_prev * calc_gdash(a_h_list[h_counter - 1], activation_func)\n",
        "        h_counter -= 2\n",
        "\n",
        "# Gradient Descent\n",
        "def gradient_descent(eta, batch_size, epoch, theta, activation_func, validation_split, loss_type, alpha,no_of_hidden_layers):\n",
        "    for itr in range(epoch):\n",
        "        # Initialize gradients and total loss\n",
        "        del_theta = [np.zeros_like(param) for param in theta]\n",
        "\n",
        "        # Iterate through the training data\n",
        "        for i in tqdm(range(int(60000 * (1 - validation_split)))):\n",
        "            # Forward propagation\n",
        "            a_h_list = forward_propagation(theta, np.float128(x_train[i].flatten().reshape((784, 1))), activation_func,no_of_hidden_layers)\n",
        "\n",
        "            # Backpropagation\n",
        "            back_propagation(a_h_list, y_train[i], np.float128((x_train[i].flatten()).reshape((784, 1))), del_theta, theta, batch_size, activation_func,no_of_hidden_layers,loss_type)\n",
        "\n",
        "            # Update weights after every mini-batch\n",
        "            if i % batch_size == 0 and i != 0:\n",
        "                for j in range(len(theta)):\n",
        "                    if(j<=no_of_hidden_layers):\n",
        "                      del_theta[j] = (del_theta[j] / batch_size)\n",
        "                    theta[j] = np.subtract(theta[j], eta * del_theta[j])-eta*alpha*theta[j]\n",
        "                    del_theta[j] = del_theta[j] * 0\n",
        "\n",
        "        # Calculate loss and accuracy\n",
        "        train_accuracy,train_loss=calc_loss_acc(theta,validation_split,activation_func,loss_type,no_of_hidden_layers,\"train\",alpha)\n",
        "        validation_accuracy,validation_loss=calc_loss_acc(theta,validation_split,activation_func,loss_type,no_of_hidden_layers,\"validation\",alpha)\n",
        "\n",
        "        # Print epoch statistics\n",
        "        wandb.log({'epoch':itr+1,\n",
        "          'loss': train_loss ,\n",
        "          'accuracy': train_accuracy,\n",
        "          'val_loss': validation_loss,\n",
        "          'val_accuracy': validation_accuracy\n",
        "      })\n",
        "\n",
        "\n",
        "# Momentum Gradient Descent\n",
        "def momentum_gradient_descent(eta, batch_size, epoch, theta, beta, activation_func, validation_split, loss_type, alpha,no_of_hidden_layers):\n",
        "    # Initialize previous history\n",
        "    prev_history = [np.zeros_like(param) for param in theta]\n",
        "\n",
        "    for itr in range(epoch):\n",
        "        del_theta = [np.zeros_like(param) for param in theta]\n",
        "\n",
        "        # Iterate through the training data\n",
        "        for i in tqdm(range(int(60000 * (1 - validation_split)))):\n",
        "            # Forward propagation\n",
        "            a_h_list = forward_propagation(theta, np.float128(x_train[i].flatten().reshape((784, 1))), activation_func,no_of_hidden_layers)\n",
        "\n",
        "            # Backpropagation\n",
        "            back_propagation(a_h_list, y_train[i], np.float128((x_train[i].flatten()).reshape((784, 1))), del_theta, theta, batch_size, activation_func,no_of_hidden_layers,loss_type)\n",
        "\n",
        "            # Update weights using momentum\n",
        "            if i % batch_size == 0 and i != 0:\n",
        "                for j in range(len(del_theta)):\n",
        "                    if(j<=no_of_hidden_layers):\n",
        "                      del_theta[j] = (del_theta[j] / batch_size)\n",
        "                    prev_history[j] = np.add(beta * prev_history[j],eta * (del_theta[j]))\n",
        "                    theta[j] = np.subtract(theta[j],  prev_history[j])-eta*alpha*theta[j]\n",
        "                    del_theta[j] = del_theta[j] * 0\n",
        "\n",
        "        # Calculate loss and accuracy\n",
        "        train_accuracy,train_loss=calc_loss_acc(theta,validation_split,activation_func,loss_type,no_of_hidden_layers,\"train\",alpha)\n",
        "        validation_accuracy,validation_loss=calc_loss_acc(theta,validation_split,activation_func,loss_type,no_of_hidden_layers,\"validation\",alpha)\n",
        "\n",
        "        # Print epoch statistics\n",
        "        wandb.log({'epoch':itr+1,\n",
        "          'loss': train_loss ,\n",
        "          'accuracy': train_accuracy,\n",
        "          'val_loss': validation_loss,\n",
        "          'val_accuracy': validation_accuracy\n",
        "      })\n",
        "\n",
        "# Nestrov Gradient Descent\n",
        "def nesterov_gradient_descent(eta, batch_size, epoch, theta, beta, activation_func, validation_split, loss_type, alpha,no_of_hidden_layers):\n",
        "    # Initialize previous history\n",
        "    prev_history = [np.zeros_like(param) for param in theta]\n",
        "\n",
        "    for itr in range(epoch):\n",
        "        del_theta = [np.zeros_like(param) for param in theta]\n",
        "\n",
        "        # Iterate through the training data\n",
        "        for i in tqdm(range(int(60000 * (1 - validation_split)))):\n",
        "            # Update weights using Nesterov accelerated gradient descent\n",
        "            updated_theta = [theta[j] - beta * prev_history[j] for j in range(len(theta))]\n",
        "\n",
        "            # Forward propagation\n",
        "            a_h_list = forward_propagation(updated_theta, np.float128(x_train[i].flatten().reshape((784, 1))), activation_func,no_of_hidden_layers)\n",
        "\n",
        "            # Backpropagation\n",
        "            back_propagation(a_h_list, y_train[i], np.float128((x_train[i].flatten()).reshape((784, 1))), del_theta, updated_theta, batch_size, activation_func,no_of_hidden_layers,loss_type)\n",
        "\n",
        "            # Update weights using momentum\n",
        "            if i % batch_size == 0 and i != 0:\n",
        "                for j in range(len(del_theta)):\n",
        "                    if(j<=no_of_hidden_layers):\n",
        "                      del_theta[j] = (del_theta[j] / batch_size)\n",
        "                    prev_history[j] = np.add(beta * prev_history[j],eta*(del_theta[j]))\n",
        "                    theta[j] = np.subtract(theta[j], prev_history[j])-eta*alpha*theta[j]\n",
        "                    del_theta[j] = del_theta[j] * 0\n",
        "\n",
        "        # Calculate loss and accuracy\n",
        "        train_accuracy,train_loss=calc_loss_acc(theta,validation_split,activation_func,loss_type,no_of_hidden_layers,\"train\",alpha)\n",
        "        validation_accuracy,validation_loss=calc_loss_acc(theta,validation_split,activation_func,loss_type,no_of_hidden_layers,\"validation\",alpha)\n",
        "\n",
        "        # Print epoch statistics\n",
        "        wandb.log({'epoch':itr+1,\n",
        "          'loss': train_loss ,\n",
        "          'accuracy': train_accuracy,\n",
        "          'val_loss': validation_loss,\n",
        "          'val_accuracy': validation_accuracy\n",
        "      })\n",
        "\n",
        "# RMS_Prop\n",
        "def rmsprop(eta, batch_size, epoch, theta, beta, eps, activation_func, validation_split, loss_type, alpha,no_of_hidden_layers):\n",
        "    # Initialize first  moment estimates\n",
        "    v_theta = [np.zeros_like(param) for param in theta]\n",
        "\n",
        "    for itr in range(epoch):\n",
        "        del_theta = [np.zeros_like(param) for param in theta]\n",
        "\n",
        "        # Iterate through the training data\n",
        "        for i in tqdm(range(int(60000 * (1 - validation_split)))):\n",
        "            # Forward propagation\n",
        "            a_h_list = forward_propagation(theta, np.float128(x_train[i].flatten().reshape((784, 1))), activation_func,no_of_hidden_layers)\n",
        "\n",
        "            # Backpropagation\n",
        "            back_propagation(a_h_list, y_train[i], np.float128((x_train[i].flatten()).reshape((784, 1))), del_theta, theta, batch_size, activation_func,no_of_hidden_layers,loss_type)\n",
        "\n",
        "            # Update weights using Adam optimizer\n",
        "            if i % batch_size == 0 and i != 0:\n",
        "                for j in range(len(theta)):\n",
        "                    if(j<=no_of_hidden_layers):\n",
        "                      del_theta[j] = (del_theta[j] / batch_size)\n",
        "                    v_theta[j] = beta * v_theta[j] + (1 - beta) * (del_theta[j] ** 2)\n",
        "\n",
        "                    # Update weights\n",
        "                    theta[j] = np.subtract(theta[j], eta * (del_theta[j]/ (np.sqrt(v_theta[j] + eps))))-eta*alpha*theta[j]\n",
        "                    del_theta[j] = del_theta[j] * 0\n",
        "\n",
        "        # Calculate loss and accuracy\n",
        "        train_accuracy,train_loss=calc_loss_acc(theta,validation_split,activation_func,loss_type,no_of_hidden_layers,\"train\",alpha)\n",
        "        validation_accuracy,validation_loss=calc_loss_acc(theta,validation_split,activation_func,loss_type,no_of_hidden_layers,\"validation\",alpha)\n",
        "\n",
        "        # Print epoch statistics\n",
        "        wandb.log({'epoch':itr+1,\n",
        "          'loss': train_loss ,\n",
        "          'accuracy': train_accuracy,\n",
        "          'val_loss': validation_loss,\n",
        "          'val_accuracy': validation_accuracy\n",
        "      })\n",
        "\n",
        "\n",
        "# Adam Optimizer\n",
        "def adam_optimizer(eta, batch_size, epoch, theta, beta1, beta2, eps, activation_func, validation_split, loss_type, alpha,no_of_hidden_layers):\n",
        "    # Initialize first and second moment estimates\n",
        "    m_theta = [np.zeros_like(param) for param in theta]\n",
        "    v_theta = [np.zeros_like(param) for param in theta]\n",
        "\n",
        "    for itr in range(epoch):\n",
        "        del_theta = [np.zeros_like(param) for param in theta]\n",
        "\n",
        "        # Iterate through the training data\n",
        "        for i in tqdm(range(int(60000 * (1 - validation_split)))):\n",
        "            # Forward propagation\n",
        "            a_h_list = forward_propagation(theta, np.float128(x_train[i].flatten().reshape((784, 1))), activation_func,no_of_hidden_layers)\n",
        "\n",
        "            # Backpropagation\n",
        "            back_propagation(a_h_list, y_train[i], np.float128((x_train[i].flatten()).reshape((784, 1))), del_theta, theta, batch_size, activation_func,no_of_hidden_layers,loss_type)\n",
        "\n",
        "            # Update weights using Adam optimizer\n",
        "            if i % batch_size == 0 and i != 0:\n",
        "                for j in range(len(theta)):\n",
        "                    if(j<=no_of_hidden_layers):\n",
        "                      del_theta[j] = (del_theta[j] / batch_size)\n",
        "                    m_theta[j] = beta1 * m_theta[j] + (1 - beta1) * del_theta[j]\n",
        "                    v_theta[j] = beta2 * v_theta[j] + (1 - beta2) * (del_theta[j] ** 2)\n",
        "\n",
        "                    # Bias correction\n",
        "                    m_hat = m_theta[j] / (1 - np.power(beta1, itr + 1))\n",
        "                    v_hat = v_theta[j] / (1 - np.power(beta2, itr + 1))\n",
        "\n",
        "                    # Update weights\n",
        "                    theta[j] = np.subtract(theta[j], eta * m_hat / (np.sqrt(v_hat) + eps))-eta*alpha*theta[j]\n",
        "                    del_theta[j] = del_theta[j] * 0\n",
        "\n",
        "         # Calculate loss and accuracy\n",
        "        train_accuracy,train_loss=calc_loss_acc(theta,validation_split,activation_func,loss_type,no_of_hidden_layers,\"train\",alpha)\n",
        "        validation_accuracy,validation_loss=calc_loss_acc(theta,validation_split,activation_func,loss_type,no_of_hidden_layers,\"validation\",alpha)\n",
        "\n",
        "        # Print epoch statistics\n",
        "        wandb.log({'epoch':itr+1,\n",
        "          'loss': train_loss ,\n",
        "          'accuracy': train_accuracy,\n",
        "          'val_loss': validation_loss,\n",
        "          'val_accuracy': validation_accuracy\n",
        "      })\n",
        "\n",
        "# nadam Optimizer\n",
        "def nadam_optimizer(eta, batch_size, epoch, theta, beta1, beta2, eps, activation_func, validation_split, loss_type, alpha ,no_of_hidden_layers):\n",
        "    # Initialize first and second moment estimates\n",
        "    m_theta = [np.zeros_like(param) for param in theta]\n",
        "    v_theta = [np.zeros_like(param) for param in theta]\n",
        "\n",
        "    for itr in range(epoch):\n",
        "        del_theta = [np.zeros_like(param) for param in theta]\n",
        "\n",
        "        # Iterate through the training data\n",
        "        for i in tqdm(range(int(60000 * (1 - validation_split)))):\n",
        "            # Forward propagation\n",
        "            a_h_list = forward_propagation(theta, np.float128(x_train[i].flatten().reshape((784, 1))), activation_func,no_of_hidden_layers)\n",
        "\n",
        "            # Backpropagation\n",
        "            back_propagation(a_h_list, y_train[i], np.float128((x_train[i].flatten()).reshape((784, 1))), del_theta, theta, batch_size, activation_func,no_of_hidden_layers,loss_type)\n",
        "\n",
        "            # Update weights using Adam optimizer\n",
        "            if i % batch_size == 0 and i != 0:\n",
        "                for j in range(len(theta)):\n",
        "                    if(j<=no_of_hidden_layers):\n",
        "                      del_theta[j] = (del_theta[j] / batch_size)\n",
        "                    m_theta[j] = beta1 * m_theta[j] + (1 - beta1) * del_theta[j]\n",
        "                    v_theta[j] = beta2 * v_theta[j] + (1 - beta2) * (del_theta[j] ** 2)\n",
        "\n",
        "                    # Bias correction\n",
        "                    m_hat = m_theta[j] / (1 - np.power(beta1, itr + 1))\n",
        "                    v_hat = v_theta[j] / (1 - np.power(beta2, itr + 1))\n",
        "\n",
        "                    # Update weights\n",
        "                    theta[j] = np.subtract(theta[j], (eta / (np.sqrt(v_hat) + eps))*((beta1*m_hat)+((1-beta1)*(del_theta[j]/(1 - np.power(beta1, itr + 1))))))-eta*alpha*theta[j]\n",
        "                    del_theta[j] = del_theta[j] * 0\n",
        "\n",
        "         # Calculate loss and accuracy\n",
        "        train_accuracy,train_loss=calc_loss_acc(theta,validation_split,activation_func,loss_type,no_of_hidden_layers,\"train\",alpha)\n",
        "        validation_accuracy,validation_loss=calc_loss_acc(theta,validation_split,activation_func,loss_type,no_of_hidden_layers,\"validation\",alpha)\n",
        "\n",
        "        # Print epoch statistics\n",
        "        wandb.log({'epoch':itr+1,\n",
        "          'loss': train_loss ,\n",
        "          'accuracy': train_accuracy,\n",
        "          'val_loss': validation_loss,\n",
        "          'val_accuracy': validation_accuracy\n",
        "      })\n",
        "\n",
        "def plotConfusionMatrix(theta,activation_func,loss_type,no_of_hidden_layers,which_loss,alpha=0):\n",
        "    classes=[\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\", \"Sneaker\",\"Bag\", \"Ankle boot\"]\n",
        "    accuracy,loss,y_true,y_pred=calc_loss_acc(theta,0,activation_func,loss_type,no_of_hidden_layers,which_loss,0)\n",
        "    print(accuracy,loss)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            plt.text(j, i, format(cm[i, j], 'd'),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    wandb.log({\"confusion_matrix\": plt})\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def run_optimizer(eta, batch_size, epoch, theta, beta, beta1, beta2, eps, activation_func, validation_split, loss_type, alpha,optimizer,no_of_hidden_layers):\n",
        "    if(optimizer==\"sgd\"):\n",
        "      gradient_descent(eta,1,epoch,theta,activation_func,validation_split,loss_type,alpha,no_of_hidden_layers)\n",
        "    elif(optimizer==\"momentum\"):\n",
        "      momentum_gradient_descent(eta,batch_size,epoch,theta,beta,activation_func,validation_split,loss_type,alpha,no_of_hidden_layers)\n",
        "    elif(optimizer==\"nesterov\"):\n",
        "      nesterov_gradient_descent(eta,batch_size,epoch,theta,beta,activation_func,validation_split,loss_type,alpha,no_of_hidden_layers)\n",
        "    elif(optimizer==\"rmsprop\"):\n",
        "      rmsprop(eta,batch_size,epoch,theta,beta,eps,activation_func,validation_split,loss_type,alpha,no_of_hidden_layers)\n",
        "    elif(optimizer==\"adam\"):\n",
        "      adam_optimizer(eta,batch_size,epoch,theta,beta1,beta2,eps,activation_func,validation_split,loss_type,alpha,no_of_hidden_layers)\n",
        "    elif(optimizer==\"nadam\"):\n",
        "      nadam_optimizer(eta,batch_size,epoch,theta,beta1,beta2,eps,activation_func,validation_split,loss_type,alpha,no_of_hidden_layers)\n",
        "\n",
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'metric': {\n",
        "        'name': 'val_accuracy',\n",
        "        'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'learning_rate': {'values': [1e-3]},\n",
        "        'batch_size': {'values': [16]},\n",
        "        'num_layers':{'values':[5]},\n",
        "        'hidden_size':{'values':[64]},\n",
        "        'epochs': {'values': [5]},\n",
        "        'weight_decay': {'values': [0.5]},\n",
        "        'optimizer': {'values': ['nesterov']},\n",
        "        'loss':{'values':['cross_entropy']},\n",
        "        'activation':{'values':['sigmoid']},\n",
        "        'weight_init':{'values':['random']}\n",
        "    }\n",
        "}\n",
        "def train():\n",
        "    # Initialize wandb\n",
        "    wandb.init()\n",
        "    # Set your hyperparameters from wandb config\n",
        "    config = wandb.config\n",
        "    wandb.run.name = f'hl_{config.num_layers}_bs_{config.batch_size}_ac_{config.activation}_opt_{config.optimizer}_lt_{config.loss}'\n",
        "\n",
        "    theta = initialize_weights(config.weight_init, 784, config.num_layers, 10, config.hidden_size)\n",
        "    # Train your model\n",
        "    run_optimizer(config.learning_rate, config.batch_size, config.epochs, theta, 0.9, 0.9, 0.999, 1e-10, config.activation, 0.1, config.loss, config.weight_decay, config.optimizer, config.num_layers)\n",
        "\n",
        "    #plot confusion matrix on best model\n",
        "    #plotConfusionMatrix(theta,config.activation,config.loss,config.num_layers,\"test\")\n",
        "# Initialize wandb sweep\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"deep_learn_assignment_1\")\n",
        "\n",
        "# Run wandb agent to execute the sweep\n",
        "wandb.agent(sweep_id, function=train, count =1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkjsPSe4DRB-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvRQkPXn48sepyzPonKUUE"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f1803ba2038b4ac2a39c086bdc1189da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f2357185363a458090f135efdba587a6",
              "IPY_MODEL_d512c0f4c7f14d7fadce98aa444aa70e"
            ],
            "layout": "IPY_MODEL_b6924ab39dec4e41af44b67b1ff221f4"
          }
        },
        "f2357185363a458090f135efdba587a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_055cd1b30acc4491a247eb5475f44542",
            "placeholder": "​",
            "style": "IPY_MODEL_42c5c55644634489bca6acd6a430fcdd",
            "value": "0.011 MB of 0.011 MB uploaded\r"
          }
        },
        "d512c0f4c7f14d7fadce98aa444aa70e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77b25017855a4b4398832a44fb197918",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_51269deaa4424633b7bac19cc5483c07",
            "value": 1
          }
        },
        "b6924ab39dec4e41af44b67b1ff221f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "055cd1b30acc4491a247eb5475f44542": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42c5c55644634489bca6acd6a430fcdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77b25017855a4b4398832a44fb197918": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51269deaa4424633b7bac19cc5483c07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}